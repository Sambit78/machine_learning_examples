{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.90|-0.90|-0.90| 1.00|\n",
      "---------------------------\n",
      "-0.90| 0.00|-0.90|-1.00|\n",
      "---------------------------\n",
      "-0.90|-0.90|-0.90|-0.90|\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  U  |\n",
      "final values:\n",
      "---------------------------\n",
      "-1.79|-0.88| 1.00| 0.00|\n",
      "---------------------------\n",
      "-2.58| 0.00|-0.72| 0.00|\n",
      "---------------------------\n",
      "-3.21|-2.79|-1.75|-1.00|\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# NOTE: this script implements the Monte Carlo Exploring-Starts method\n",
    "#       for finding the optimal policy\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "\n",
    "  # reset game to start at a random position\n",
    "  # we need to do this if we have a deterministic policy\n",
    "  # we would never end up at certain states, but we still want to measure their value\n",
    "  # this is called the \"exploring starts\" method\n",
    "  start_states = list(grid.actions.keys())\n",
    "  start_idx = np.random.choice(len(start_states))\n",
    "  grid.set_state(start_states[start_idx])\n",
    "\n",
    "  s = grid.current_state()\n",
    "  a = np.random.choice(ALL_POSSIBLE_ACTIONS) # first action is uniformly random\n",
    "\n",
    "  # be aware of the timing\n",
    "  # each triple is s(t), a(t), r(t)\n",
    "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "  states_actions_rewards = [(s, a, 0)]\n",
    "  seen_states = set()\n",
    "  seen_states.add(grid.current_state())\n",
    "  num_steps = 0\n",
    "  while True:\n",
    "    r = grid.move(a)\n",
    "    num_steps += 1\n",
    "    s = grid.current_state()\n",
    "\n",
    "    if s in seen_states:\n",
    "      # hack so that we don't end up in an infinitely long episode\n",
    "      # bumping into the wall repeatedly\n",
    "      # if num_steps == 1 -> bumped into a wall and haven't moved anywhere\n",
    "      #   reward = -10\n",
    "      # else:\n",
    "      #   reward = falls off by 1 / num_steps\n",
    "      reward = -10. / num_steps\n",
    "      states_actions_rewards.append((s, None, reward))\n",
    "      break\n",
    "    elif grid.game_over():\n",
    "      states_actions_rewards.append((s, None, r))\n",
    "      break\n",
    "    else:\n",
    "      a = policy[s]\n",
    "      states_actions_rewards.append((s, a, r))\n",
    "    seen_states.add(s)\n",
    "\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_actions_returns = []\n",
    "  first = True\n",
    "  for s, a, r in reversed(states_actions_rewards):\n",
    "    # the value of the terminal state is 0 by definition\n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_actions_returns.append((s, a, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_actions_returns\n",
    "\n",
    "\n",
    "def max_dict(d):\n",
    "  # returns the argmax (key) and max (value) from a dictionary\n",
    "  # put this into a function since we are using it so often\n",
    "  max_key = None\n",
    "  max_val = float('-inf')\n",
    "  for k, v in d.items():\n",
    "    if v > max_val:\n",
    "      max_val = v\n",
    "      max_key = k\n",
    "  return max_key, max_val\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  # grid = standard_grid()\n",
    "  # try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
    "  # in order to minimize number of steps\n",
    "  grid = negative_grid(step_cost=-0.9)\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # initialize a random policy\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initialize Q(s,a) and returns\n",
    "  Q = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions: # not a terminal state\n",
    "      Q[s] = {}\n",
    "      for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0 # needs to be initialized to something so we can argmax it\n",
    "        returns[(s,a)] = []\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      pass\n",
    "\n",
    "  # repeat until convergence\n",
    "  deltas = []\n",
    "  for t in range(2000):\n",
    "    if t % 100 == 0:\n",
    "      print(t)\n",
    "\n",
    "    # generate an episode using pi\n",
    "    biggest_change = 0\n",
    "    states_actions_returns = play_game(grid, policy)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "      # check if we have already seen s\n",
    "      # called \"first-visit\" MC policy evaluation\n",
    "      sa = (s, a)\n",
    "      if sa not in seen_state_action_pairs:\n",
    "        old_q = Q[s][a]\n",
    "        returns[sa].append(G)\n",
    "        Q[s][a] = np.mean(returns[sa])\n",
    "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "        seen_state_action_pairs.add(sa)\n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "    # update policy\n",
    "    for s in policy.keys():\n",
    "      policy[s] = max_dict(Q[s])[0]\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  print(\"final policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # find V\n",
    "  V = {}\n",
    "  for s, Qs in Q.items():\n",
    "    V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "  print(\"final values:\")\n",
    "  print_values(V, grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
