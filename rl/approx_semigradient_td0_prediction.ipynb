{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "from td0_prediction import play_game, SMALL_ENOUGH, GAMMA, ALPHA, ALL_POSSIBLE_ACTIONS\n",
    "\n",
    "# NOTE: this is only policy evaluation, not optimization\n",
    "\n",
    "class Model:\n",
    "  def __init__(self):\n",
    "    self.theta = np.random.randn(4) / 2\n",
    "  \n",
    "  def s2x(self, s):\n",
    "    return np.array([s[0] - 1, s[1] - 1.5, s[0]*s[1] - 3, 1])\n",
    "\n",
    "  def predict(self, s):\n",
    "    x = self.s2x(s)\n",
    "    return self.theta.dot(x)\n",
    "\n",
    "  def grad(self, s):\n",
    "    return self.s2x(s)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "  }\n",
    "\n",
    "  model = Model()\n",
    "  deltas = []\n",
    "\n",
    "  # repeat until convergence\n",
    "  k = 1.0\n",
    "  for it in range(20000):\n",
    "    if it % 10 == 0:\n",
    "      k += 0.01\n",
    "    alpha = ALPHA/k\n",
    "    biggest_change = 0\n",
    "\n",
    "    # generate an episode using pi\n",
    "    states_and_rewards = play_game(grid, policy)\n",
    "    # the first (s, r) tuple is the state we start in and 0\n",
    "    # (since we don't get a reward) for simply starting the game\n",
    "    # the last (s, r) tuple is the terminal state and the final reward\n",
    "    # the value for the terminal state is by definition 0, so we don't\n",
    "    # care about updating it.\n",
    "    for t in range(len(states_and_rewards) - 1):\n",
    "      s, _ = states_and_rewards[t]\n",
    "      s2, r = states_and_rewards[t+1]\n",
    "      # we will update V(s) AS we experience the episode\n",
    "      old_theta = model.theta.copy()\n",
    "      if grid.is_terminal(s2):\n",
    "        target = r\n",
    "      else:\n",
    "        target = r + GAMMA*model.predict(s2)\n",
    "      model.theta += alpha*(target - model.predict(s))*model.grad(s)\n",
    "      biggest_change = max(biggest_change, np.abs(old_theta - model.theta).sum())\n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  # obtain predicted values\n",
    "  V = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions:\n",
    "      V[s] = model.predict(s)\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      V[s] = 0\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
